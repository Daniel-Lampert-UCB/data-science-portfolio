# 4. Another Turnout Question

We're sorry; it is just that the outcome and treatment spaces are so clear! 

Hill and Kousser (2015) report that it is possible to increase the probability that someone votes in the California *Primary Election* simply by sending them a letter in the mail. This is kind of surprising, because who even reads the mail anymore anyways? (Actually, if you talk with folks who work in the space, they'll say, "We know that everybody throws our mail away; we just hope they see it on the way to the garbage.")

Can you replicate their findings? Let's walk through them.

```{r}
number_rows <- 3872268 # you should change this for your answer. full points for full data

d <- data.table::fread(
  input = 'https://ucb-mids-w241.s3-us-west-1.amazonaws.com/hill_kousser_analysis_file.csv', 
  nrows = number_rows)

```

(As an aside, you'll note that this takes some time to download. One idea is to save a copy locally, rather than continuing to read from the internet. One problem with this idea is that you might be tempted to make changes to this cannonical data; changes that wouldn't be reflected if you were to ever pull a new copy from the source tables. One method of dealing with this is proposed by [Cookiecutter data science](https://drivendata.github.io/cookiecutter-data-science/#links-to-related-projects-and-references).)

Here's what is in that data. 

- `age.bin` a bucketed, descriptive, version of the `age.in.14` variable 
- `party.bin` a bucketed version of the `Party` variable 
- `in.toss.up.dist` whether the voter lives in a district that often has close races 
- `minority.dist` whether the voter lives in a majority minority district, i.e. a majority black, latino or other racial/ethnic minority district 
- `Gender` voter file reported gender
- `Dist1-8` congressional and data districts 
- `reg.date.pre.08` whether the voter has been registered since before 2008 
- `vote.xx.gen` whether the voter voted in the `xx` general election 
- `vote.xx.gen.pri` whether the voter voted in the `xx` general primary election 
- `vote.xx.pre.pri` whether the voter voted in the `xx` presidential primary election 
- `block.num` a block indicator for blocked random assignment. 
- `treatment.assign` either "Control", "Election Info", "Partisan Cue", or "Top-Two Info"
- `yvar` the outcome variable: did the voter vote in the 2014 primary election 

These variable names are horrible. Do two things: 

- Rename the smallest set of variables that you think you might use to something more useful. (You can use `data.table::setnames` to do this.) 
- For the variables that you think you might use; check that the data makes sense; 

```{r}
d <- setnames(d, c('age.bin', 'party.bin', 'in.toss.up.dist', 'minority.dist', 'Gender', 'Dist1-8', 
                   'reg.date.pre.08', 'vote.xx.gen', 'vote.xx.gen.pri', 'vote.xx.pre.pri', 
                   'block.num', 'treatment.assign', 'yvar'), c('age_group', 'party_bin', 
                                                               'swing_district', 'majority_minority', 
                                                               'gender', 'district', 'registered_pre_08','voted_20_general', 
                                                               'voted_20_primary', 'voted_20_pres_primary', 'block_no', 'treatment', 'voted'),skip_absent=TRUE)
dplyr::count(d, age_group, sort = TRUE)
dplyr::count(d, party_bin, sort = TRUE)
dplyr::count(d, voted, sort = TRUE)
dplyr::count(d, swing_district, sort = TRUE)
dplyr::count(d, treatment, sort = TRUE)
```
> What do the age groups signify? Seems like there is an issue with the treatment variable. 

When you make these changes, take care to make these changes in a way that is reproducible. In doing so, ensure that nothing is positional indexed, since the orders of columns might change in the source data). 

While you're at it, you might as well also modify your `.gitignore` to ignore the data folder. Because you're definitely going to have the data rejected when you try to push it to github. And every time that happens, it is a 30 minute rabbit hole to try and re-write git history. 

## Some questions! 

1. **A Simple Treatment Effect**: Load the data and estimate a `lm` model that compares the rates of turnout in the control group to the rate of turnout among anybody who received *any* letter. This model combines all the letters into a single condition -- "treatment" compared to a single condition "control". Report robust standard errors, and include a narrative sentence or two after your code.  

```{r effect of receiving a letter} 
d[, treatment_binary := ifelse(treatment != 'Control', 1, 0)]
model_simple <- d[, lm(voted ~ treatment_binary)]
coeftest(model_simple, vcov = vcovHC(model_simple, type = "HC0"))
coefci(model_simple, level = 0.95, vcov. = vcovHC, 'treatment_binary')
```
> The results for this model are highly significant with a P-value well below the 0.05 significance level. This indicates that sending letters does in fact have an impact on voting turnout. I included heterogenous robust standard errors and confidence intervals in case the residuals are not normally distributed arround the regression line.  

2. **Specific Treatment Effects**: Suppose that you want to know whether different letters have different effects. To begin, what are the effects of each of the letters, as compared to control? Estimate an appropriate linear model and use robust standard errors. 

```{r effect of receiving specific letters} 
model_letters <- d[, lm(voted ~ treatment)]
coeftest(model_letters, vcov = vcovHC(model_letters, type = "HC0"))
coefci(model_letters, level = 0.95, vcov. = vcovHC)
```
> The coefficient for each one of the treatments is actually very small with the partisan treatment having the biggest effect. The partisan treatment increases voter turnout by 0.525%. However, each coefficient is highly significant.

3. Does the increased flexibilitiy of a different treatment effect for each of the letters improve the performance of the model? Test, using an F-test. What does the evidence suggest, and what does this mean about whether there **are** or **are not** different treatment effects for the different letters?

```{r f-test}
model_anova <- anova(model_simple, model_letters, test = "F")
model_anova
```
> The more complex model does not increase performance. The P-value for the more complex model is not significant and the RSS has stayed the same. This means the increased complexity in the second model does not lead to any improvements. 

4. **More Specific Treatment Effects** Is one message more effective than the others? The authors have drawn up this design as a full-factorial design. Write a *specific* test for the difference between the *Partisan* message and the *Election Info* message. Write a *specific* test for the difference between *Top-Two Info* and the *Election Info* message. Report robust standard errors on both tests and include a short narrative statement after your estimates. 

```{r specific treatment effects}
model_partisan_vs_info <- d[treatment == 'Partisan' | treatment == 'Election info',lm(voted ~ treatment) ]
model_partisan_vs_info_se <- coeftest(model_partisan_vs_info, vcov = vcovHC(model_partisan_vs_info, type = "HC0"))
model_partisan_vs_info_ci <- coefci(model_partisan_vs_info, level = 0.95, vcov. = vcovHC)
model_partisan_vs_info_se
model_partisan_vs_info_ci

model_top_two_vs_info  <- d[treatment == "Top-two info" | treatment == "Election info", lm(voted ~ treatment)]
model_top_two_vs_info_se <- coeftest(model_top_two_vs_info, vcov = vcovHC(model_top_two_vs_info, type = "HC0"))
model_top_two_vs_info_ci <- coefci(model_top_two_vs_info, level = 0.95, vcov. = vcovHC)
model_top_two_vs_info_se
model_top_two_vs_info_ci
```
> For `model_partisan_vs_info` the coefficient for `treatmentPartisan` is slightly larger than for `Election info`. However the results are not significant. For `model_top_two_vs_info` the coefficient for `Top-two info` is slightly smaller than it is for `Election info` however the results are also not significant. This can also be seen in the confidence interval which contains zero.

5. **Blocks? We don't need no stinking blocks?**  The blocks in this data are defined in the `block.num` variable (which you may have renamed). There are a *many* of blocks in this data, none of them are numerical -- they're all category indicators. How many blocks are there? 

> There are **`r d[, uniqueN(block_no)]` block ids**. This is a lot of blocks!

6. **SAVE YOUR CODE FIRST** but then try to estimate a `lm` that evaluates the effect of receiving *any letter*, and includes this block-level information. What happens? Why do you think this happens? If this estimate *would have worked* (that's a hint that we don't think it will), what would the block fixed effects have accomplished?

```{r going down with the ship!}
#commented it out since it crashes
#model_block_fx  <- d[,lm(voted ~ treatment_binary + as.factor(block_no))]
```
> We would be able to see if different blocks have an impact on voter turnout. Unfortunately their design includes far too many blocks and therefore this is impossible to do.

6. Even though we can't estimate this fixed effects model directly, we can get the same information and model improvement if we're *just a little bit clever*. Create a new variable that is the *average turnout within a block* and attach this back to the data.table. Use this new variable in a regression that regresses voting on `any_letter` and this new `block_average`. Then, using an F-test, does the increased information from all these blocks improve the performance of the *causal* model? Use an F-test to check. 

```{r alternate approach}
d[, mean_turnout_block := mean(voted), keyby = block_no]
model_block_average <- d[, lm(voted ~ treatment_binary+ mean_turnout_block)]
coeftest(model_block_average, vcov = vcovHC(model_block_average, type = "HC0"))
f_test_results <- anova(model_simple, model_block_average, test = 'F')
f_test_results

```
> This does seem to improve the performance of the model. The p-value for the model with all the additional blocks is highly significant. This is indicative that the block fixed effects are in fact predictive of turnout. 

7. Doesn't this feel like using a bad-control in your regression? Has the treatment coefficient changed from when you didn't include the `block_average` measure to when you did? Have the standard errors on the treatment coefficient changed from when you didn't include the `block_average` measure to when you did? Why is this OK to do?

> The coefficient for the treatment did change but to a very small degree. From what I know about the blocking mechanism I do not think it's a bad control. I do not think the blocking variable is associated with both the treatment and the outcome. However it does feel kind of strange since it includes the average turnout per block which feels like it's associated with the outcome. Surprisingly the standdard errors for the model with the block average turnout did not change. I believe it is acceptable to include these values because they are not associated with both the outcome and the treatment. They are just associated with the outcome. Therefore, this does not open a back door pathway.  



